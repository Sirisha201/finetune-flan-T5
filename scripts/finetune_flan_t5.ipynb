{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "790a4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa17f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (/home/jupyter/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3f7133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b4c1756d2a48e2a8e11e9478521fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf18bb9b033a4bc085e111e14aca5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eac25c9bb8f41ff916fd99cc7a34c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_id= \"google/flan-t5-xl\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\",load_in_8bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9cc4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration  \n",
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "pl.seed_everything(100)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb69c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                      conversations\n",
      "0  0  [{'from': 'human', 'value': 'Hey Samantha, I'v...\n",
      "1  1  [{'from': 'human', 'value': 'Hey Samantha, I'v...\n",
      "2  2  [{'from': 'human', 'value': 'Hello Samantha, I...\n",
      "3  3  [{'from': 'human', 'value': 'Hey Samantha, I'v...\n",
      "4  4  [{'from': 'human', 'value': 'Hello Samantha, a...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://huggingface.co/datasets/ehartford/samantha-data/resolve/main/samantha-1.1.json\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "json_data = pd.json_normalize(data)  # Adjust depending on the structure of your JSON data\n",
    "\n",
    "print(json_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba43896",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "# Iterate through the DataFrame to reformat the data\n",
    "for _, row in json_data.iterrows():\n",
    "    conversations = row['conversations']\n",
    "    for i in range(0, len(conversations) - 1):\n",
    "        if i + 1 < len(conversations):\n",
    "            questions.append(conversations[i]['value'])\n",
    "            answers.append(conversations[i + 1]['value'])\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({'question': questions, 'answer': answers})\n",
    "new_data.to_csv('reformatted_data_reversed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13000865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31420\n"
     ]
    }
   ],
   "source": [
    "size=round(len(new_data)*0.5)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4f304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = new_data[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "736220b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "INPUT_MAX_LEN = 128 #input length\n",
    "OUTPUT_MAX_LEN = 128 # output length\n",
    "TRAIN_BATCH_SIZE = 2 # batch size of training\n",
    "VAL_BATCH_SIZE = 2 # batch size for validation\n",
    "EPOCHS = 5 # number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d995dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, how are you today?\" \n",
    "input_tokenize = tokenizer( \n",
    "             text,\n",
    "            add_special_tokens=True,        #Add Special tokens like [CLS] and [SEP]\n",
    "            max_length=128,\n",
    "            padding = 'max_length',         #for padding to max_length for equal sequence length\n",
    "            truncation = True,              #truncate the text if it is greater than max_length\n",
    "            return_attention_mask=True,     #will return attention mask\n",
    "            return_tensors=\"pt\"             #return tensor formate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4ce0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class T5Dataset:\n",
    "    def __init__(self,question,answer):   \n",
    "\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_max_len = INPUT_MAX_LEN\n",
    "        self.output_max_len = OUTPUT_MAX_LEN\n",
    "  \n",
    "    def __len__(self):                      # This method retrives the number of item from the dataset\n",
    "        return len(self.question)\n",
    "\n",
    "    def __getitem__(self,item):             # This method retrieves the item at the specified index item. \n",
    "\n",
    "        question = str(self.question[item])\n",
    "        question = ''.join(question.split())\n",
    "\n",
    "        answer = str(self.answer[item])\n",
    "        answer = ''.join(answer.split())\n",
    "\n",
    "        input_tokenize = self.tokenizer(      \n",
    "                question,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.input_max_len,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        output_tokenize = self.tokenizer(\n",
    "                answer,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.output_max_len,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\"\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "        input_ids = input_tokenize[\"input_ids\"].flatten()\n",
    "        attention_mask = input_tokenize[\"attention_mask\"].flatten()\n",
    "        labels = output_tokenize['input_ids'].flatten()\n",
    "\n",
    "        out = {\n",
    "                'question':question,      \n",
    "                'answer':answer,\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask':attention_mask,\n",
    "                'target':labels\n",
    "            }\n",
    "\n",
    "        return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15f780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5DataLoad(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,df_train,df_test):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_max_len = INPUT_MAX_LEN\n",
    "        self.out_max_len = OUTPUT_MAX_LEN\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        self.train_data = T5Dataset(\n",
    "            question = self.df_train.question.values,\n",
    "            answer = self.df_train.answer.values\n",
    "        )\n",
    "        \n",
    "        self.valid_data = T5Dataset(\n",
    "            question = self.df_test.question.values,\n",
    "            answer = self.df_test.answer.values\n",
    "        )\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "         self.train_data,\n",
    "         batch_size= TRAIN_BATCH_SIZE,\n",
    "         shuffle=True, \n",
    "         num_workers=2\n",
    "        )\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "        self.valid_data,\n",
    "        batch_size= VAL_BATCH_SIZE,\n",
    "        num_workers = 2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72cbf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Model(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\",load_in_8bit=True, return_dict=True)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        output = self.model(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        labels=labels\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels= batch[\"target\"]\n",
    "        loss, logits = self(input_ids , attention_mask, labels)\n",
    "\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels= batch[\"target\"]\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a6ec394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6169e54b793a4a1589d0091691e0cb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "befbd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    df_train, df_test = train_test_split(reduced_data,test_size = 0.2, random_state=100)\n",
    "    dataload = T5DataLoad(df_train,df_test)\n",
    "    dataload.setup()\n",
    "    device = DEVICE\n",
    "    model = T5Model()\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=\"/home/jupyter/notebooks/flan_t5_model/\",\n",
    "        filename='best-model',\n",
    "        save_top_k=2,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks = checkpoint,\n",
    "        max_epochs= 1,\n",
    "        accelerator=\"gpu\"\n",
    "    )\n",
    "    trainer.fit(model, dataload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ee82f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e341e9c9eebd4101b0932d6168d07567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 2.8 B \n",
      "-----------------------------------------------------\n",
      "635 M     Trainable params\n",
      "2.2 B     Non-trainable params\n",
      "2.8 B     Total params\n",
      "11,399.029Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e796c54c1f9475bbe5bf6a8661c1088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 7541: 'val_loss' reached 0.80771 (best 0.80771), saving model to '/home/jupyter/notebooks/flan_t5_model/best-model.ckpt' as top 2\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = T5Model.load_from_checkpoint('/home/jupyter/notebooks/flan_t5_model/best-model.ckpt')\n",
    "train_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d90eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(question):\n",
    "\n",
    "    inputs_encoding =  tokenizer(\n",
    "        question,\n",
    "        add_special_tokens=True,\n",
    "        max_length= INPUT_MAX_LEN,\n",
    "        padding = 'max_length',\n",
    "        truncation='only_first',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    \n",
    "    generate_ids = train_model.model.generate(\n",
    "        input_ids = inputs_encoding[\"input_ids\"],\n",
    "        attention_mask = inputs_encoding[\"attention_mask\"],\n",
    "        max_length = INPUT_MAX_LEN,\n",
    "        num_beams = 4,\n",
    "        num_return_sequences = 1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    preds = [\n",
    "        tokenizer.decode(gen_id,\n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=True)\n",
    "        for gen_id in generate_ids\n",
    "    ]\n",
    "\n",
    "    return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428b15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ques:  hi, how are you doing?\n",
      "BOT:  I'mgladtoheartyou!It'sbeenapleasingtalkingtoyou,andI'malwaysheretodiscussanythingyou'dliketosharewithme.Rememberthatmypurposeistoprovidecompanionshipandemotionalsupportwithintheboundariesofourfriendship.\n"
     ]
    }
   ],
   "source": [
    "ques = \"hi, how are you doing?\"\n",
    "print(\"Ques: \",ques)\n",
    "print(\"BOT: \",generate_question(ques))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
